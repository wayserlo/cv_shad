{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQg-V9XQ2T_f"
   },
   "source": [
    "# Generative Adversarial Networks\n",
    "\n",
    "Generative Adversarial Networks (GANs) are an approach to generative modeling based on deep learning methods.\n",
    "\n",
    "The standard problem settings for GANs are generation of photorealistic images and image-to-image translation tasks (translating photos of summer to winter, day to night etc.).\n",
    "\n",
    "In this task you will familiarize yourself with both these problems while trying to create fake images of sneakers. \n",
    "\n",
    "As GANs still do have certain limitations about generating large images, the task is decomposed into two: first, use a simple GAN to generate a bunch of low resolution images from noise, then upscale them using another generative model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-1cXZG02T_g"
   },
   "source": [
    "This notebook is built around [PyTorch](https://pytorch.org/) and [Lightning](https://pytorchlightning.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_lightning\n",
      "  Downloading pytorch_lightning-2.1.2-py3-none-any.whl (776 kB)\n",
      "\u001b[K     |████████████████████████████████| 776 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.9/site-packages (from pytorch_lightning) (23.1)\n",
      "Collecting PyYAML>=5.4\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-macosx_10_9_x86_64.whl (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /opt/miniconda3/lib/python3.9/site-packages (from pytorch_lightning) (4.62.3)\n",
      "Collecting typing-extensions>=4.0.0\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting torch>=1.12.0\n",
      "  Downloading torch-2.1.2-cp39-none-macosx_10_9_x86_64.whl (147.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 147.0 MB 3.6 MB/s eta 0:00:011   |███▏                            | 14.3 MB 2.7 MB/s eta 0:00:50\n",
      "\u001b[?25hCollecting fsspec[http]>2021.06.0\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[K     |████████████████████████████████| 168 kB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lightning-utilities>=0.8.0\n",
      "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
      "Collecting torchmetrics>=0.7.0\n",
      "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
      "\u001b[K     |████████████████████████████████| 806 kB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /opt/miniconda3/lib/python3.9/site-packages (from pytorch_lightning) (1.26.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.9.1-cp39-cp39-macosx_10_9_x86_64.whl (397 kB)\n",
      "\u001b[K     |████████████████████████████████| 397 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/miniconda3/lib/python3.9/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.27.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.2.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-macosx_10_9_x86_64.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-macosx_10_9_x86_64.whl (29 kB)\n",
      "Collecting async-timeout<5.0,>=4.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp39-cp39-macosx_10_9_x86_64.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 4.7 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/lib/python3.9/site-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (58.0.4)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.9/site-packages (from torch>=1.12.0->pytorch_lightning) (3.2.1)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.7 MB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/miniconda3/lib/python3.9/site-packages (from torch>=1.12.0->pytorch_lightning) (3.12.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/miniconda3/lib/python3.9/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.9/site-packages (from jinja2->torch>=1.12.0->pytorch_lightning) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/miniconda3/lib/python3.9/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/miniconda3/lib/python3.9/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.9/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2023.7.22)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 5.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: multidict, mpmath, frozenlist, yarl, typing-extensions, sympy, jinja2, fsspec, async-timeout, aiosignal, torch, lightning-utilities, aiohttp, torchmetrics, PyYAML, pytorch-lightning\n",
      "Successfully installed PyYAML-6.0.1 aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 fsspec-2023.12.2 jinja2-3.1.2 lightning-utilities-0.10.0 mpmath-1.3.0 multidict-6.0.4 pytorch-lightning-2.1.2 sympy-1.12 torch-2.1.2 torchmetrics-1.2.1 typing-extensions-4.9.0 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "E4XlI2qi2T_g"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -d data/ ]; then\n",
    "    curl -sO 'https://code.mipt.ru/courses-public/cv/storage/-/raw/tasks/sneaker-generation/data.zip'\n",
    "    unzip -qo data.zip\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LMNVXiEZ2T_g"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a69fa653480c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "IMAGE_SIZE = (28, 28)\n",
    "NOISE_DIM = 100\n",
    "LOW_RES_SIZE = IMAGE_SIZE\n",
    "HIGH_RES_SIZE = (112, 112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8nVxCxC2T_g",
    "outputId": "1d0f6fc8-a945-4600-9a68-05c10c5429ad"
   },
   "outputs": [],
   "source": [
    "train_kwargs = {}\n",
    "dataset_root = \"./data\"\n",
    "images_dir = \"images\"\n",
    "image_filenames = sorted(os.listdir(os.path.join(dataset_root, images_dir)))\n",
    "len(image_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akxy294r2T_h"
   },
   "source": [
    "# I. Image Generation\n",
    "\n",
    "Your first task is to solve a problem of generating photorealistic images out of noise (okay, this might sound optimistic).\n",
    "\n",
    "Namely, you are required to **create fake images of sneakers of resolution 28x28 given a vector of noise sampled from standard normal distribution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEeSWf-t2T_h"
   },
   "source": [
    "Here is simple PyTorch `Dataset` class for loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bh6UclOg2T_h"
   },
   "outputs": [],
   "source": [
    "class SneakersDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        images_dir=\"images\",\n",
    "        input_size=None,\n",
    "        target_size=None,\n",
    "    ):\n",
    "        self.images_dir = os.path.join(root_dir, images_dir)\n",
    "        self.input_size = input_size\n",
    "        self.target_size = target_size\n",
    "        files = os.listdir(self.images_dir)\n",
    "        self.all_images = sorted([file for file in files if file.endswith(\".jpg\")])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.images_dir, self.all_images[idx])\n",
    "        image_bgr = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        if self.input_size is not None:\n",
    "            input_rgb = cv2.resize(image_rgb, self.input_size)\n",
    "            input_rgb = (input_rgb / 255).astype(np.float32)\n",
    "        if self.target_size is not None:\n",
    "            image_rgb = cv2.resize(image_rgb, self.target_size)\n",
    "        image_rgb = (image_rgb / 255).astype(np.float32)\n",
    "\n",
    "        # tanh values in [-1, 1]\n",
    "        image_rgb = torch.from_numpy(image_rgb * 2 - 1)\n",
    "        if self.input_size is not None:\n",
    "            input_rgb = torch.from_numpy(input_rgb * 2 - 1)\n",
    "            return input_rgb, image_rgb\n",
    "        else:\n",
    "            return image_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miRyiLHc2T_h"
   },
   "source": [
    "Here we create `LightningDataModule` which will handle dataset loading in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tePuYxj2T_h"
   },
   "outputs": [],
   "source": [
    "class SneakersGANDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str, batch_size, shuffle=True):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.dataset = SneakersDataset(self.data_dir, target_size=IMAGE_SIZE)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2MfJORX2T_h"
   },
   "outputs": [],
   "source": [
    "def data2img(d: np.ndarray):\n",
    "    return 0.5 * d + 0.5\n",
    "\n",
    "\n",
    "def visualize_images(data, n_rows, n_cols):\n",
    "    n_samples = n_rows * n_cols\n",
    "\n",
    "    if len(data) != n_samples:\n",
    "        sample_indices = np.random.choice(\n",
    "            len(data),\n",
    "            n_samples,\n",
    "            replace=len(data) < n_samples,\n",
    "        )\n",
    "    else:\n",
    "        sample_indices = np.arange(len(data)).astype(int)\n",
    "\n",
    "    plt.figure(figsize=(int(2.5 * n_cols), int(2.5 * n_rows)))\n",
    "    for i, sample_index in enumerate(sample_indices):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        plt.imshow(data2img(data[sample_index]))\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "rFngx2hW2T_h",
    "outputId": "a24a69cb-fea8-4bd4-9dba-e4212e0fa586"
   },
   "outputs": [],
   "source": [
    "dm = SneakersGANDataModule(\"data\", batch_size=32)\n",
    "dm.setup()\n",
    "visualize_images(next(iter(dm.train_dataloader())), 4, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ixooZ_e2T_h"
   },
   "source": [
    "## Fully connected GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aR9nD0Oy2T_i"
   },
   "source": [
    "### Generator (0.5 pts) [cross-check:0]\n",
    "\n",
    "Our first step is to build a generator. You should use the layers in `torch.nn` (imported in the beginning as `nn`) to construct the model. Since we are using PyTorch and Lightning, you should create `nn.Module`. Use the default initializers for parameters.\n",
    "\n",
    "Architecture:\n",
    " * Fully connected (`Linear` in PyTorch) with output size of 1024\n",
    " * ReLU\n",
    " * Fully connected with output size of 1024\n",
    " * ReLU\n",
    " * Fully connected with output size of 28 x 28 x 3\n",
    " * TanH (to restrict every element of the output to be in the range [-1,1])\n",
    " * Reshape into (28, 28, 3)\n",
    "\n",
    "> You could perform reshaping inside of `forward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ek4myHCp2T_i"
   },
   "outputs": [],
   "source": [
    "class FCGenerator(nn.Module):\n",
    "    def __init__(self, noise_dim: int, img_shape: tuple):\n",
    "        super().__init__()\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # TODO: add other layers\n",
    "            ...,\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        # keep batch size\n",
    "        img = img.view(img.size(0), *self.img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4Ln0kZM2T_i"
   },
   "outputs": [],
   "source": [
    "fc_gen = FCGenerator(NOISE_DIM, IMAGE_SIZE + (3,))\n",
    "img_generated = fc_gen(torch.randn(32, NOISE_DIM))\n",
    "assert img_generated.shape[1:] == IMAGE_SIZE + (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "You1A1iK2T_i"
   },
   "source": [
    "### Discriminator (0.5 pts) [cross-check:1]\n",
    "\n",
    "Now you are to build a discriminator. You should use the default initializers for parameters here as well.\n",
    "\n",
    "Architecture:\n",
    " * Flatten\n",
    " * Fully connected with output size of 256\n",
    " * Leaky ReLU(0.01)\n",
    " * Fully connected with output size of 256\n",
    " * Leaky ReLU(0.01)\n",
    " * Fully connected with output size of 1\n",
    " * Sigmoid (to obtain logits as an output)\n",
    "\n",
    "The output of the discriminator should thus have shape `[batch_size, 1]`, and contain real numbers corresponding to the probability that each of the `batch_size` inputs is a real image.\n",
    "\n",
    "> You could perform flattening inside of `forward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DyiCu7cn2T_i"
   },
   "outputs": [],
   "source": [
    "class FCDiscriminator(nn.Module):\n",
    "    def __init__(self, img_shape: tuple):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # TODO: add layers\n",
    "            ...\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        return self.model(img_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhCMNZ_k2T_i"
   },
   "outputs": [],
   "source": [
    "fc_dis = FCDiscriminator(IMAGE_SIZE + (3,))\n",
    "prob_dis = fc_dis(img_generated)\n",
    "assert prob_dis.shape[1:] == (1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjPcLoqF2T_i"
   },
   "source": [
    "## GAN Loss (1.0 pts) [cross-check:2]\n",
    "\n",
    "Compute the generator and discriminator loss. The generator loss is:\n",
    "$$\\ell_G  =  -\\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]$$\n",
    "and the discriminator loss is:\n",
    "$$ \\ell_D = -\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] - \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "\n",
    "Instead of computing the expectation, you may average over elements of the minibatch, so make sure to combine the loss by *averaging* instead of summing.\n",
    "\n",
    "Note that these are negated from the equations presented earlier as we will be *minimizing* these losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a `LightningModule` with all necessary methods. You should fill all `TODO` comments.\n",
    "\n",
    "> Don't forget to use `detach` method for discriminator training step to prevent backpropagation for generator compuational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "30a0jfdl2T_i"
   },
   "outputs": [],
   "source": [
    "class FCGAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        width,\n",
    "        height,\n",
    "        channels,\n",
    "        noise_dim=100,\n",
    "        lr=0.0002,\n",
    "        b1=0.5,\n",
    "        b2=0.999,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # make <arg> available as self.hparams.<arg>\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Important: This property activates manual optimization.\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        img_shape = (width, height, channels)\n",
    "        self.generator = FCGenerator(\n",
    "            noise_dim=self.hparams.noise_dim,\n",
    "            img_shape=img_shape,\n",
    "        )\n",
    "        self.discriminator = FCDiscriminator(\n",
    "            img_shape=img_shape,\n",
    "        )\n",
    "\n",
    "        self.validation_z = torch.randn(8, noise_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def gan_loss(self, y_hat, y):\n",
    "        # TODO: implement GAN loss (hint: binary cross entropy)\n",
    "        return ...\n",
    "\n",
    "    def training_step(self, imgs, batch_idx):\n",
    "        opt_g, opt_d = self.optimizers()\n",
    "\n",
    "        z = torch.randn(imgs.size(0), self.hparams.noise_dim)\n",
    "        # move to same device as imgs\n",
    "        z = z.type_as(imgs)\n",
    "\n",
    "        # optimize generator\n",
    "        self.generated_imgs = self(z)\n",
    "\n",
    "        # all fake, but we want to be real\n",
    "        valid = torch.ones(imgs.size(0), 1)\n",
    "        valid = valid.type_as(imgs)\n",
    "\n",
    "        g_loss = self.gan_loss(self.discriminator(self(z)), valid)\n",
    "        self.log(\"g_loss\", g_loss, prog_bar=True)\n",
    "\n",
    "        opt_g.zero_grad()\n",
    "        self.manual_backward(g_loss)\n",
    "        opt_g.step()\n",
    "\n",
    "        # optimize discriminator\n",
    "        valid = torch.ones(imgs.size(0), 1)\n",
    "        valid = valid.type_as(imgs)\n",
    "        # TODO: loss for `discriminator(imgs)` and `valid`\n",
    "        real_loss = ...\n",
    "\n",
    "        # TODO: zero vector for fake_loss computation\n",
    "        fake = ...\n",
    "        fake = fake.type_as(imgs)\n",
    "\n",
    "        fake_loss = self.gan_loss(self.discriminator(self(z).detach()), fake)\n",
    "\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        self.log(\"d_loss\", d_loss, prog_bar=True)\n",
    "\n",
    "        opt_d.zero_grad()\n",
    "        self.manual_backward(d_loss)\n",
    "        opt_d.step()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # send results for validation_z to TensorBoard\n",
    "        z = self.validation_z.type_as(self.generator.model[0].weight)\n",
    "        # channels before pixels\n",
    "        sample_imgs = self(z).permute(0, 3, 1, 2)\n",
    "        grid = torchvision.utils.make_grid(sample_imgs)\n",
    "        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95JM5baZmMKi"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DK1-PrBF2T_i"
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs[\"max_epochs\"] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287,
     "referenced_widgets": [
      "8bea138c3ab442bb86b744f810d71767",
      "ee1598bfc7db4ba4bea30daaf4bf0611",
      "c50de042d4ef4ae19f0be884b1acc898",
      "bfa3287788244810a88ae475155cde83",
      "219c85a79f7a4c27865536ef90360b84",
      "eebaddb891ac4356af67f46d931acff0",
      "2fc48807178e4b1daa98cae6335ba274",
      "50c7495608d44921a0c729609106459c"
     ]
    },
    "id": "AHcwyJFA2T_i",
    "outputId": "f01571d6-72b2-4bbb-a444-6bda59ceee4d"
   },
   "outputs": [],
   "source": [
    "dm = SneakersGANDataModule(\"data\", batch_size=BATCH_SIZE)\n",
    "model = FCGAN(*IMAGE_SIZE, 3, NOISE_DIM)\n",
    "trainer = pl.Trainer(**train_kwargs)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "IxsBIlLm2T_i",
    "outputId": "0d48fe1e-6f7a-42da-cefb-b67410ec034c"
   },
   "outputs": [],
   "source": [
    "visualize_images(model(torch.randn(32, NOISE_DIM)).detach(), 4, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hint: this architecture isn't great, so don't spend all your time to train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9r8wLBjp2T_i"
   },
   "source": [
    "## Deep Convolutional GANs\n",
    "In the first part of the notebook, you have implemented an almost direct copy of the original GAN network from Ian Goodfellow. However, this network architecture allows no real spatial reasoning. It is unable to reason about things like \"sharp edges\" in general because it lacks any convolutional layers. Thus, in this section, you are to implement some of the ideas from [DCGAN](https://arxiv.org/abs/1511.06434), where both discriminator and generator are convolutional networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlmSNMvu2T_i"
   },
   "source": [
    "### Generator (1.0 pts) [cross-check:3]\n",
    "\n",
    "Architecture:\n",
    " * Fully connected with output size of 128x7x7\n",
    " * Reshape into (128, 7, 7)\n",
    " * ReLU\n",
    " * UpSampling2D(2)\n",
    " * Conv2D: 3x3, filters=128, padding=\"same\"\n",
    " * Batch Normalization 2D with momentum(0.8)\n",
    " * ReLU\n",
    " * UpSampling2D(2)\n",
    " * Conv2D: 3x3, filters=64, padding=\"same\"\n",
    " * Batch Normalization 2D with momentum(0.8)\n",
    " * ReLU\n",
    " * Conv2D: 3x3, filters=3, padding=\"same\"\n",
    " * TanH (to restrict every element of the output to be in the range [-1,1])\n",
    "\n",
    "> There is no `padding=\"same\"` in PyTorch, but you could use `padding=k//2` and `padding_mode=\"zeros\"` for kernel size `k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "px1zHOt02T_i"
   },
   "outputs": [],
   "source": [
    "class DCGenerator(nn.Module):\n",
    "    def __init__(self, noise_dim: int, img_shape: tuple):\n",
    "        super().__init__()\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # TODO: add layers\n",
    "            ...\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        # make channel axis last\n",
    "        img = img.permute(0, 2, 3, 1)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VlTQU7d2T_i"
   },
   "outputs": [],
   "source": [
    "dc_gen = DCGenerator(NOISE_DIM, IMAGE_SIZE + (3,))\n",
    "fake_imgs = dc_gen(torch.randn(10, NOISE_DIM))\n",
    "assert fake_imgs.shape[1:] == IMAGE_SIZE + (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyqq0l5l2T_i"
   },
   "source": [
    "### Discriminator (1.0 pts) [cross-check:4]\n",
    "\n",
    "Architecture:\n",
    " * Conv2D: 3x3, filters=32, strides=2, padding=\"same\"\n",
    " * Leaky ReLU(0.2)\n",
    " * Dropout(0.25)\n",
    " * Conv2D: 3x3, filters=64, strides=2, padding=\"same\"\n",
    " * Zero Padding 2D: ((0, 1), (0, 1))\n",
    " * Batch Normalization 2D with momentum(0.8)\n",
    " * Leaky ReLU(0.2)\n",
    " * Dropout(0.25)\n",
    " * Conv2D: 3x3, filters=128, strides=2, padding=\"same\"\n",
    " * Batch Normalization 2D with momentum(0.8)\n",
    " * Leaky ReLU(0.2)\n",
    " * Dropout(0.25)\n",
    " * Conv2D: 3x3, filters=256, strides=2, padding=\"same\"\n",
    " * Batch Normalization 2D with momentum(0.8)\n",
    " * Leaky ReLU(0.2)\n",
    " * Dropout(0.25)\n",
    " * Flatten\n",
    " * Fully connected layer with output size 1\n",
    " * Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeqlNeKM2T_i"
   },
   "outputs": [],
   "source": [
    "class DCDiscriminator(nn.Module):\n",
    "    def __init__(self, img_shape: tuple):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # TODO: add layers\n",
    "            ...\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # channels first\n",
    "        img_chan = img.permute(0, 3, 1, 2)\n",
    "        return self.model(img_chan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iv8LAYu52T_i"
   },
   "outputs": [],
   "source": [
    "dc_dis = DCDiscriminator(img_shape=IMAGE_SIZE + (3,))\n",
    "fake_proba = dc_dis(fake_imgs)\n",
    "assert fake_proba.shape[1:] == (1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EE-qWG0Y2T_i"
   },
   "source": [
    "## Least Squares GAN loss (1.0 pts) [cross-check:5]\n",
    "We'll now look at [Least Squares GAN loss](https://arxiv.org/abs/1611.04076), a newer, more stable alternative to the original GAN loss function. For this part, all you have to do is change the loss function and retrain the model. You'll implement equation (9) in the paper, with the generator loss:\n",
    "$$\\ell_G  =  \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[\\left(D(G(z))-1\\right)^2\\right]$$\n",
    "and the discriminator loss:\n",
    "$$ \\ell_D = \\frac{1}{2}\\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\left(D(x)-1\\right)^2\\right] + \\frac{1}{2}\\mathbb{E}_{z \\sim p(z)}\\left[ \\left(D(G(z))\\right)^2\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pjHNyLm2T_i"
   },
   "outputs": [],
   "source": [
    "class DCGAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        width,\n",
    "        height,\n",
    "        channels,\n",
    "        noise_dim=100,\n",
    "        lr=0.0002,\n",
    "        b1=0.5,\n",
    "        b2=0.999,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # make <arg> available as self.hparams.<arg>\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Important: This property activates manual optimization.\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        img_shape = (width, height, channels)\n",
    "        self.generator = DCGenerator(\n",
    "            noise_dim=self.hparams.noise_dim,\n",
    "            img_shape=img_shape,\n",
    "        )\n",
    "        self.discriminator = DCDiscriminator(\n",
    "            img_shape=img_shape,\n",
    "        )\n",
    "\n",
    "        self.validation_z = torch.randn(8, noise_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def ls_loss(self, y_hat, y):\n",
    "        # TODO: implement least squares GAN loss\n",
    "        return ...\n",
    "\n",
    "    def training_step(self, imgs, batch_idx):\n",
    "        opt_g, opt_d = self.optimizers()\n",
    "\n",
    "        z = torch.randn(imgs.size(0), self.hparams.noise_dim)\n",
    "        # move to same device as imgs\n",
    "        z = z.type_as(imgs)\n",
    "\n",
    "        # optimize generator\n",
    "        # TODO: compute loss as before (FCGAN)\n",
    "        ...\n",
    "\n",
    "        self.log(\"g_loss\", g_loss, prog_bar=True)\n",
    "\n",
    "        opt_g.zero_grad()\n",
    "        self.manual_backward(g_loss)\n",
    "        opt_g.step()\n",
    "\n",
    "        # optimize discriminator\n",
    "        # TODO: compute loss as before (FCGAN)\n",
    "        ...\n",
    "\n",
    "        self.log(\"d_loss\", d_loss, prog_bar=True)\n",
    "\n",
    "        opt_d.zero_grad()\n",
    "        self.manual_backward(d_loss)\n",
    "        opt_d.step()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        z = self.validation_z.type_as(self.generator.model[0].weight)\n",
    "        # channels before pixels\n",
    "        sample_imgs = self(z).permute(0, 3, 1, 2)\n",
    "        grid = torchvision.utils.make_grid(sample_imgs)\n",
    "        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmQLyGSv2T_i"
   },
   "source": [
    "### Training\n",
    "Train generator and discriminator in a loop and draw results once every N iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs[\"max_epochs\"] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287,
     "referenced_widgets": [
      "0ec98bb8bd094c67b978ff1c1431cf35",
      "eb09c5f038cc4168ad38f0311efa1b37",
      "65c0759e8c4b4a0c98ac100a1d2fe52c",
      "3e88cfc14c264875a8bfccf795556ec1",
      "87efc1da8c3b4879a1148935a73fb1b7",
      "cf90e5e319d446d8a4a62c941cad401e",
      "46b4a2f955d7409eb66d6565eabac132",
      "2300102631604b4d9d3cea1c2356ab99"
     ]
    },
    "id": "M4R3uSOJ2T_i",
    "outputId": "c905ab47-e385-4046-ab23-fda9edd1abab"
   },
   "outputs": [],
   "source": [
    "dm = SneakersGANDataModule(\"data\", batch_size=BATCH_SIZE)\n",
    "model = DCGAN(*IMAGE_SIZE, 3, NOISE_DIM)\n",
    "trainer = pl.Trainer(**train_kwargs)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEAcxvr7nDqE"
   },
   "source": [
    "Prepare data for **III. GAN metrics. PRD score**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rirxBd_wQikH"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "def predict(\n",
    "    model: pl.LightningModule,\n",
    "    data: torch.Tensor,\n",
    "    batch_size: int,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    output = []\n",
    "    slicer = range(0, len(data), batch_size)\n",
    "    if verbose:\n",
    "        slicer = tqdm.tqdm(slicer)\n",
    "\n",
    "    model.eval()\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    if use_gpu:\n",
    "        model.cuda()\n",
    "    with torch.no_grad():\n",
    "        for i in slicer:\n",
    "            x = data[i : i + batch_size]\n",
    "            if use_gpu:\n",
    "                x = x.cuda()\n",
    "            y = model(x)\n",
    "            if use_gpu:\n",
    "                y = y.cpu()\n",
    "            output.append(y)\n",
    "    if use_gpu:\n",
    "        model.cpu()\n",
    "\n",
    "    output = torch.cat(output, dim=0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vAWj43ixmQnk",
    "outputId": "ba132299-8962-4771-a48c-aabd58174c4e"
   },
   "outputs": [],
   "source": [
    "dataset = SneakersDataset(\"data\", target_size=IMAGE_SIZE)\n",
    "lr_data = torch.stack([dataset[i] for i in range(len(dataset))], dim=0)\n",
    "dc_fake_data = predict(model, torch.randn(lr_data.size(0), NOISE_DIM), BATCH_SIZE)\n",
    "print(\"LR data:\", lr_data.size())\n",
    "print(\"Fake LR data:\", dc_fake_data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "rFEC7ayo2T_i",
    "outputId": "758c5fb7-e5f1-4a06-8808-fee0a87610ee"
   },
   "outputs": [],
   "source": [
    "visualize_images(dc_fake_data, 4, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NarPWs-M2T_j"
   },
   "source": [
    "# II. Super Resolution\n",
    "\n",
    "In this part of the notebook you will train a generative model that solves an image-to-image problem, with \"small images\" as a source domain and \"large images\" being a target domain. \n",
    "\n",
    "To specify the task, you are to **scale small images of 28x28 pixels up to size of 112x112 pixels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wdq-1LSb2T_j"
   },
   "outputs": [],
   "source": [
    "class SneakersSRDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str, batch_size, shuffle=True):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.dataset = SneakersDataset(\n",
    "            self.data_dir,\n",
    "            input_size=LOW_RES_SIZE,\n",
    "            target_size=HIGH_RES_SIZE,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 808
    },
    "id": "tITnKMbz2T_j",
    "outputId": "d644421f-572c-4116-a4ca-656fe0fc1de4"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "dm = SneakersSRDataModule(\"data\", batch_size=batch_size)\n",
    "dm.setup()\n",
    "real_data_lr, real_data_hr = next(iter(dm.train_dataloader()))\n",
    "\n",
    "assert real_data_lr.shape[1:] == LOW_RES_SIZE + (3,)\n",
    "assert real_data_hr.shape[1:] == HIGH_RES_SIZE + (3,)\n",
    "\n",
    "plt.figure(figsize=(8, 14))\n",
    "for i in range(batch_size):\n",
    "    plt.subplot(batch_size, 2, 2 * i + 1)\n",
    "    plt.title(\"Low resolution\")\n",
    "    plt.imshow(data2img(real_data_lr[i]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(batch_size, 2, 2 * i + 2)\n",
    "    plt.title(\"High resolution\")\n",
    "    plt.imshow(data2img(real_data_hr[i]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Vu7Kd2vHkTW"
   },
   "source": [
    "In the second part of this task, you are to train an [SRGAN](https://arxiv.org/abs/1609.04802)-like model. For your convinience, some layers are already implemented. Now it's your turn -- fill the gaps so the model passes the asserts below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XgweaOiH8Ue"
   },
   "source": [
    "### Generator (1.5 pts) [cross-check:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmTyXzOqIBzQ"
   },
   "source": [
    "To build a SRGAN Generator, you will need a basic Residual Block(filters):\n",
    "\n",
    "* Conv2D: 3x3, filters=filters, strides=1, padding=\"same\"\n",
    "* ReLU\n",
    "* Batch Normalization 2D with momentum(0.8)\n",
    "* Conv2D: 3x3, filters=filters, strides=1, padding=\"same\"\n",
    "* Batch Normalization 2D with momentum(0.8)\n",
    "* Sum up outputs with inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcHFj_MEKEAy"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, filters: int):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # TODO: add layers\n",
    "            ...\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z) + z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADazxfW1KEMY"
   },
   "source": [
    "Upsampling Block(filters):\n",
    "\n",
    "* UpSampling2D(2)\n",
    "* Conv2D: 3x3, filters=filters, strides=1, padding=\"same\"\n",
    "* ReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEqo5dKaKEcv"
   },
   "outputs": [],
   "source": [
    "class UpsamplingBlock(nn.Module):\n",
    "    def __init__(self, filters: int, input_filters: int = None):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # TODO: add layers\n",
    "            ...\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNK9xG5NKEq_"
   },
   "source": [
    "Now, using these basic building blocks, one is able to define a SRGAN generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mPTVp1hG9aQ"
   },
   "outputs": [],
   "source": [
    "class SRGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.init_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=9, padding=9 // 2, padding_mode=\"zeros\"),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.residual_chain = nn.Sequential(\n",
    "            *[ResidualBlock(64) for _ in range(16)],\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=3 // 2, padding_mode=\"zeros\"),\n",
    "            nn.BatchNorm2d(64, momentum=0.8)\n",
    "        )\n",
    "        self.upsample_conv = nn.Sequential(\n",
    "            UpsamplingBlock(256, input_filters=64),\n",
    "            UpsamplingBlock(256),\n",
    "            nn.Conv2d(256, 3, kernel_size=9, padding=9 // 2, padding_mode=\"zeros\"),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = z.permute(0, 3, 1, 2)\n",
    "        conv = self.init_conv(x)\n",
    "        x = self.residual_chain(conv) + conv\n",
    "        img = self.upsample_conv(x)\n",
    "        # make channel axis last\n",
    "        img = img.permute(0, 2, 3, 1)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KWElBB1Sld-"
   },
   "outputs": [],
   "source": [
    "real_data_lr, real_data_hr = next(iter(dm.train_dataloader()))\n",
    "fake_hr = SRGenerator()(real_data_lr)\n",
    "assert fake_hr.shape == real_data_hr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyoU_b6bXvAN"
   },
   "source": [
    "### Discriminator (1.5 pts) [cross-check:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXjBXT93Xx75"
   },
   "source": [
    "First, define a Discriminator Block(filters, strides):\n",
    "\n",
    "* Conv2D: 3x3, filters=filters, strides=strides, padding=\"same\"\n",
    "* Leaky ReLU(0.2)\n",
    "* (optional) Batch Normalization 2D with momentum(0.8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzwB98UXWDYM"
   },
   "outputs": [],
   "source": [
    "class DBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_filters: int,\n",
    "        filters: int,\n",
    "        strides: int,\n",
    "        batch_norm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            # TODO: add layers (except BatchNorm2d)\n",
    "            ...\n",
    "        ]\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(filters, momentum=0.8))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9XySSJen22I"
   },
   "source": [
    "Now, use this block to build up SRGAN discriminator:\n",
    "\n",
    "* DBlock(filters=64, strides=1) with Batch Normalization\n",
    "* DBlock(filters=64, strides=2)\n",
    "* DBlock(filters=128, strides=1)\n",
    "* DBlock(filters=128, strides=2)\n",
    "* DBlock(filters=256, strides=1)\n",
    "* DBlock(filters=256, strides=2)\n",
    "* DBlock(filters=512, strides=1)\n",
    "* DBlock(filters=512, strides=2)\n",
    "* Flatten\n",
    "* Fully connected with output size of 1024\n",
    "* Leaky ReLU(0.2)\n",
    "* Fully connected with output size of 1\n",
    "* Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TE1_Ea-Zn3Po"
   },
   "outputs": [],
   "source": [
    "class SRDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # TODO: add layers\n",
    "            ...\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = z.permute(0, 3, 1, 2)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RaAKs9BMp47b"
   },
   "outputs": [],
   "source": [
    "fake_probas = SRDiscriminator()(fake_hr)\n",
    "assert fake_probas.shape[1:] == (1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MS1yeB5cquyO"
   },
   "source": [
    "Typically, SRGAN is trained with additional loss on features from pretrained VGG-19. However, you task will be a bit simplier: use **mean squared error** between real and fake data instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SaydO1FJqA0G"
   },
   "outputs": [],
   "source": [
    "class SRGAN(pl.LightningModule):\n",
    "    def __init__(self, validation_lr):\n",
    "        super().__init__()\n",
    "\n",
    "        # Important: This property activates manual optimization.\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.generator = SRGenerator()\n",
    "        self.discriminator = SRDiscriminator()\n",
    "\n",
    "        self.validation_lr = validation_lr\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def gan_loss(self, y_hat, y):\n",
    "        # TODO: implement loss\n",
    "        return ...\n",
    "\n",
    "    def ls_loss(self, y_hat, y):\n",
    "        # TODO: implement loss\n",
    "        return ...\n",
    "\n",
    "    def mse_loss(self, sr_imgs, hr_imgs):\n",
    "        # TODO: implement MSE loss\n",
    "        return ...\n",
    "\n",
    "    def training_step(self, imgs, batch_idx):\n",
    "        opt_g, opt_d = self.optimizers()\n",
    "        lr_imgs, hr_imgs = imgs\n",
    "\n",
    "        # optimize generator\n",
    "        sr_imgs = self(lr_imgs)\n",
    "\n",
    "        # all fake, but we want to be real\n",
    "        valid = torch.ones(lr_imgs.size(0), 1)\n",
    "        valid = valid.type_as(lr_imgs)\n",
    "\n",
    "        gan_loss = self.gan_loss(self.discriminator(sr_imgs), valid)\n",
    "        mse_loss = self.mse_loss(sr_imgs, hr_imgs)\n",
    "\n",
    "        g_loss = 1 * mse_loss + 0.5 * gan_loss\n",
    "        self.log(\"g_loss\", g_loss, prog_bar=True)\n",
    "        self.log(\"gan_loss\", gan_loss, prog_bar=True)\n",
    "        self.log(\"mse_loss\", mse_loss, prog_bar=True)\n",
    "\n",
    "        opt_g.zero_grad()\n",
    "        self.manual_backward(g_loss)\n",
    "        opt_g.step()\n",
    "\n",
    "        # optimize discriminator\n",
    "        valid = torch.ones(hr_imgs.size(0), 1)\n",
    "        valid = valid.type_as(hr_imgs)\n",
    "        real_loss = self.ls_loss(self.discriminator(hr_imgs), valid)\n",
    "\n",
    "        fake = torch.zeros(lr_imgs.size(0), 1)\n",
    "        fake = fake.type_as(lr_imgs)\n",
    "        fake_loss = self.ls_loss(self.discriminator(self(lr_imgs).detach()), fake)\n",
    "\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        self.log(\"d_loss\", d_loss, prog_bar=True)\n",
    "\n",
    "        opt_d.zero_grad()\n",
    "        self.manual_backward(d_loss)\n",
    "        opt_d.step()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_g = torch.optim.Adam(\n",
    "            self.generator.parameters(),\n",
    "            lr=2e-4,\n",
    "            betas=(0.5, 0.999),\n",
    "        )\n",
    "        opt_d = torch.optim.Adam(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=1e-3,\n",
    "            betas=(0.5, 0.999),\n",
    "        )\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        z = self.validation_lr.type_as(self.generator.init_conv[0].weight)\n",
    "        # channels before pixels\n",
    "        sample_imgs = self(z).permute(0, 3, 1, 2)\n",
    "        grid = torchvision.utils.make_grid(sample_imgs)\n",
    "        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 855
    },
    "id": "71X-grsBcsAt",
    "outputId": "a34fcaa3-a23d-4db1-b689-8f17c944775b"
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KmvmfcDvXSy"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xWztpoFse_y"
   },
   "outputs": [],
   "source": [
    "train_kwargs[\"max_epochs\"] = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341,
     "referenced_widgets": [
      "53c288cd0241447aba7eb03d30512fbf",
      "85c3b1a8cd174ec9ba6f0c7f9b6bb98c",
      "881713e41393412d95ee3207e34e40d5",
      "ace5dc252dec4d1ab33a3e9a47b20d59",
      "9436c1b792ce4d7bb44367a85c0cbe1a",
      "42da303a422d4800926b2e80d7b5a26b",
      "b8c94a1ad9e34924984c7c1b44becccc",
      "72767f3c61574b7dbe080a01bcdace49"
     ]
    },
    "id": "qrGJI_gTuJcR",
    "outputId": "da63a74c-d295-4102-d115-db616e1e6074"
   },
   "outputs": [],
   "source": [
    "dm = SneakersSRDataModule(\"data\", batch_size=16)\n",
    "dm.setup()\n",
    "model = SRGAN(validation_lr=next(iter(dm.train_dataloader()))[0][:4])\n",
    "trainer = pl.Trainer(**train_kwargs)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuWpdw7_vbxu"
   },
   "source": [
    "Prepare data for **III. GAN metrics**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZC8EP_xvhVD",
    "outputId": "3b7f90b6-97ba-4f40-b733-16a0b95d3dc6"
   },
   "outputs": [],
   "source": [
    "dm = SneakersSRDataModule(\"data\", batch_size=BATCH_SIZE)\n",
    "dm.setup()\n",
    "hr_data = []\n",
    "for lr, hr in dm.train_dataloader():\n",
    "    hr_data.append(hr)\n",
    "hr_data = torch.cat(hr_data, dim=0)\n",
    "\n",
    "batch_size = 16\n",
    "sr_fake_data_from_real_lr = predict(model, lr_data, batch_size)\n",
    "sr_fake_data_from_fake_lr = predict(model, dc_fake_data, batch_size)\n",
    "print(\"HR data:\", hr_data.size())\n",
    "print(\"SR from real LR:\", sr_fake_data_from_real_lr.size())\n",
    "print(\"SR from fake LR:\", sr_fake_data_from_fake_lr.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "kmLKMN_rujEn",
    "outputId": "d0f93d9b-378e-4131-fe37-81d91185ebc9"
   },
   "outputs": [],
   "source": [
    "visualize_images(sr_fake_data_from_real_lr, 4, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "mhlAvVbpM9a9",
    "outputId": "0a1f1cb9-4eeb-497f-cdbf-6459f621f5fa"
   },
   "outputs": [],
   "source": [
    "visualize_images(sr_fake_data_from_fake_lr, 4, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auJUUp_ArHkb"
   },
   "source": [
    "# III. GAN metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sr1sSMURrPLf"
   },
   "source": [
    "There exists a few metrics used to measure GAN performance. Some of them are based on comparing real samples against generated ones, while the other rely on additional pretrained models that are applied to both real and generated data in order to accumulate high-level statistics. In this task, you are going to use two metrics representing these two approaches -- namely, [Precision-Recall Density](https://arxiv.org/pdf/1806.00035) and [Fréchet Inception Distance](https://arxiv.org/pdf/1706.08500)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJDHoskhrMso"
   },
   "source": [
    "##  Precision-Recall Density (PRD score) (1.0 pts) [cross-check:8]\n",
    "\n",
    "Your first task is to implement [Precision-Recall Density](https://arxiv.org/pdf/1806.00035.pdf) score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_oUr0sjrOti"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "\n",
    "def bin_counts(real_data, fake_data, n_bins=25):\n",
    "    real_data = real_data.reshape(len(real_data), -1)\n",
    "    fake_data = fake_data.reshape(len(fake_data), -1)\n",
    "\n",
    "    data = np.vstack([real_data, fake_data])\n",
    "\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_bins, n_init=10).fit(data)\n",
    "\n",
    "    real_labels = kmeans.labels_[: len(real_data)]\n",
    "    fake_labels = kmeans.labels_[len(real_data) :]\n",
    "\n",
    "    real_density, _ = np.histogram(\n",
    "        real_labels,\n",
    "        bins=n_bins,\n",
    "        range=[0, n_bins],\n",
    "        density=True,\n",
    "    )\n",
    "    # TODO: same for fake_labels\n",
    "    fake_density, _ = ...\n",
    "\n",
    "    return real_density, fake_density\n",
    "\n",
    "\n",
    "def sample_bin_counts(real_data, fake_data, n_bins=25, repeat_number=10, verbose=True):\n",
    "    real_densities = []\n",
    "    fake_densities = []\n",
    "    counter = range(repeat_number)\n",
    "    if verbose:\n",
    "        counter = tqdm.tqdm(counter)\n",
    "    for _ in counter:\n",
    "        real, fake = bin_counts(real_data, fake_data, n_bins=n_bins)\n",
    "        real_densities.append(real)\n",
    "        fake_densities.append(fake)\n",
    "    return np.array(real_densities).mean(axis=0), np.array(fake_densities).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2RUvLSmpeB3"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "def calculate_alpha_beta(real_density, fake_density, n_thetas=1000):\n",
    "    assert real_density.shape == fake_density.shape\n",
    "\n",
    "    alpha = []\n",
    "    beta = []\n",
    "\n",
    "    thetas = np.linspace(1e-6, np.pi / 2 - 1e-6, num=n_thetas)\n",
    "    for theta in thetas:\n",
    "        tan = math.tan(theta)\n",
    "        # TODO: implement paper formula\n",
    "        alpha.append(...)\n",
    "        beta.append(...)\n",
    "\n",
    "    return alpha, beta\n",
    "\n",
    "\n",
    "def calculate_prd_score(real_data, fake_data):\n",
    "    # Calculate bin counts from real and generated data multiple times\n",
    "    # TODO\n",
    "    real_density, fake_density = ...\n",
    "\n",
    "    plt.bar(\n",
    "        range(len(real_density)),\n",
    "        real_density,\n",
    "        width=1,\n",
    "        color=\"g\",\n",
    "        alpha=0.5,\n",
    "        label=\"Real density\",\n",
    "    )\n",
    "    plt.bar(\n",
    "        range(len(fake_density)),\n",
    "        fake_density,\n",
    "        width=1,\n",
    "        color=\"r\",\n",
    "        alpha=0.5,\n",
    "        label=\"Fake density\",\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate alpha and beta\n",
    "    # TODO\n",
    "    alpha, beta = ...\n",
    "\n",
    "    # Calculate area under curve (AUC) for alpha and beta\n",
    "    # TODO\n",
    "    score = ...\n",
    "\n",
    "    return score, alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXk1fvzBpnAw"
   },
   "source": [
    "Calculate PRD score for DCGAN (task I). You should pass `lr_data` and `dc_fake_data` to scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "Wya_v6L5pn_3",
    "outputId": "7e418a2e-e3cd-4424-b5ce-80b6ad74b71f"
   },
   "outputs": [],
   "source": [
    "score, _, _ = calculate_prd_score(\n",
    "    lr_data,\n",
    "    dc_fake_data,\n",
    ")\n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2zud1znprDY"
   },
   "source": [
    "Now use PRD score to compare high resolution data generated from real low resolution data (`sr_fake_data_from_real_lr`) and fake resolution data (`sr_fake_data_from_fake_lr`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RStA_RiapsEt",
    "outputId": "45c89aa2-fdfe-410d-89d5-2a5731248a1d"
   },
   "outputs": [],
   "source": [
    "print(\"Generated from real LR\")\n",
    "score_real, alpha_real, beta_real = calculate_prd_score(\n",
    "    hr_data,\n",
    "    sr_fake_data_from_real_lr,\n",
    ")\n",
    "print(\"Score:\", score_real, end=\"\\n\\n\")\n",
    "\n",
    "print(\"Generated from fake LR\")\n",
    "score_fake, alpha_fake, beta_fake = calculate_prd_score(\n",
    "    hr_data,\n",
    "    sr_fake_data_from_fake_lr,\n",
    ")\n",
    "print(\"Score:\", score_fake, end=\"\\n\\n\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.tick_params(axis=\"both\", which=\"major\", labelsize=8)\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.plot(alpha_real, beta_real, color=\"g\", label=\"Generated from real LR\")\n",
    "plt.plot(alpha_fake, beta_fake, color=\"r\", label=\"Generated from fake LR\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3qk_19kR_zr"
   },
   "source": [
    "## Fréchet Inception Distance (FID score) (1.0 pts) [cross-check:9]\n",
    "\n",
    "[Frechet Inception Distance](https://arxiv.org/pdf/1706.08500) is an improved version of [Inception score](https://arxiv.org/abs/1606.03498), that additionally calculates the statistics of real data and compares it to the statistics of generated data. It is probably the most widely-used option for evaluating GANs, and relies on features extracted with [InceptionV3](https://arxiv.org/abs/1512.00567) pretrained on ImageNet. These features assumed to come from a multivariate Gaussian distribution, so Fréchet distance between two multivariate Gaussians can be calculated:\n",
    "\n",
    "$$\\text{FID} = ||\\mu_r - \\mu_g||^2 + \\text{Tr} (\\Sigma_r + \\Sigma_g - 2 (\\Sigma_r \\Sigma_g)^{1/2}),$$\n",
    "\n",
    "where $X_r \\sim \\mathcal{N} (\\mu_r, \\Sigma_r)$ and $X_g \\sim \\mathcal{N} (\\mu_g, \\Sigma_g)$ are the 2048-dimensional activations of the Inception-v3 pool3 layer for real and generated samples respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcHc-r8oSGj7"
   },
   "source": [
    "First, create InceptionV3 ([from torch repository](https://pytorch.org/hub/pytorch_vision_inception_v3/)) model. As you will be using it for feature extraction only, we should remove last fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d01uHGsrVzWR"
   },
   "outputs": [],
   "source": [
    "class InceptionHeadless(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "        self.model = torch.hub.load(\"pytorch/vision\", \"inception_v3\", pretrained=True)\n",
    "        # remove last fc layer\n",
    "        self.model.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = z.permute(0, 3, 1, 2)\n",
    "        x = self.upsample(x)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3U0MbG9KzIMn",
    "outputId": "4b8982f1-30ec-410e-ad12-ec1b684370b0"
   },
   "outputs": [],
   "source": [
    "inception = InceptionHeadless()\n",
    "inception.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    assert inception(hr_data[:32]).shape[1:] == (2048,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILNxlRbP25Ec"
   },
   "outputs": [],
   "source": [
    "def calculate_activations(data, batch_size=32, verbose=False):\n",
    "    # Calculate activations of Pool3 layer of InceptionV3\n",
    "    if verbose:\n",
    "        print(\"Calculating activations...\")\n",
    "    activations = predict(inception, data, batch_size=32)\n",
    "    return activations\n",
    "\n",
    "\n",
    "def calculate_activation_statistics(activations):\n",
    "    # Calculate mean and covariance of activations. Mind the dimensions!\n",
    "    # TODO\n",
    "    mu = ...\n",
    "    t = activations - mu\n",
    "    # TODO\n",
    "    sigma = ...\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ohaJKXnVzIS8",
    "outputId": "37286764-2908-41ce-eb14-350297b9f0b0"
   },
   "outputs": [],
   "source": [
    "real_activations = calculate_activations(hr_data, verbose=True)\n",
    "real_mu, real_sigma = calculate_activation_statistics(real_activations)\n",
    "\n",
    "assert real_mu.shape == (2048,)\n",
    "assert real_sigma.shape == (2048, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYG7z9qSWcyA"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    assert mu1.shape == mu2.shape\n",
    "    assert sigma1.shape == sigma2.shape\n",
    "\n",
    "    sigma1_sigma2 = scipy.linalg.sqrtm(np.dot(sigma1, sigma2))\n",
    "\n",
    "    # Numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(sigma1_sigma2):\n",
    "        sigma1_sigma2 = sigma1_sigma2.real\n",
    "\n",
    "    # Product might be almost singular\n",
    "    if not np.isfinite(sigma1_sigma2).all():\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        sigma1_sigma2 = scipy.linalg.sqrtm(np.dot(sigma1 + offset, sigma2 + offset))\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    # use diff, sigma1, sigma2 to calculate FID according to the formula above\n",
    "    # TODO: implement score from paper\n",
    "    return ...\n",
    "\n",
    "\n",
    "def calculate_fid_score(real_data, fake_data, verbose=False):\n",
    "    # Run inception on real and fake data to obtain activations\n",
    "    # TODO\n",
    "    real_activations = ...\n",
    "    fake_activations = ...\n",
    "\n",
    "    # Calculate mu and sigma for both real and fake activations\n",
    "    # TODO\n",
    "    real_mu, real_sigma = ...\n",
    "    fake_mu, fake_sigma = ...\n",
    "\n",
    "    # Calculate Frechet distance\n",
    "    return calculate_frechet_distance(\n",
    "        real_mu,\n",
    "        real_sigma,\n",
    "        fake_mu,\n",
    "        fake_sigma,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQjxyGZLewHo"
   },
   "source": [
    "Calculate FID score between `hr_data` and `sr_fake_data_from_real_hr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvHBjRgOeuCx",
    "outputId": "aee25821-4f46-4d72-d820-1826ac0beca9"
   },
   "outputs": [],
   "source": [
    "score = calculate_fid_score(\n",
    "    hr_data,\n",
    "    sr_fake_data_from_real_lr,\n",
    ")\n",
    "print(\"Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHYWDEwZeyT9"
   },
   "source": [
    "Putting it all together: calculate FID score between `hr_data` and `sr_fake_data_from_fake_hr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgyhdAyoezVL",
    "outputId": "514544ab-b513-4521-9ee9-a0f20a731cb2"
   },
   "outputs": [],
   "source": [
    "score = calculate_fid_score(\n",
    "    hr_data,\n",
    "    sr_fake_data_from_fake_lr,\n",
    ")\n",
    "print(\"Score:\", score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GAN_pytorch_04.01.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ec98bb8bd094c67b978ff1c1431cf35": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_65c0759e8c4b4a0c98ac100a1d2fe52c",
       "IPY_MODEL_3e88cfc14c264875a8bfccf795556ec1"
      ],
      "layout": "IPY_MODEL_eb09c5f038cc4168ad38f0311efa1b37"
     }
    },
    "219c85a79f7a4c27865536ef90360b84": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2300102631604b4d9d3cea1c2356ab99": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fc48807178e4b1daa98cae6335ba274": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e88cfc14c264875a8bfccf795556ec1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2300102631604b4d9d3cea1c2356ab99",
      "placeholder": "​",
      "style": "IPY_MODEL_46b4a2f955d7409eb66d6565eabac132",
      "value": " 45/45 [28:47&lt;00:00, 38.39s/it, loss=0.142, v_num=1, g_loss=0.216, d_loss=0.0969]"
     }
    },
    "42da303a422d4800926b2e80d7b5a26b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46b4a2f955d7409eb66d6565eabac132": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "50c7495608d44921a0c729609106459c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53c288cd0241447aba7eb03d30512fbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_881713e41393412d95ee3207e34e40d5",
       "IPY_MODEL_ace5dc252dec4d1ab33a3e9a47b20d59"
      ],
      "layout": "IPY_MODEL_85c3b1a8cd174ec9ba6f0c7f9b6bb98c"
     }
    },
    "65c0759e8c4b4a0c98ac100a1d2fe52c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Epoch 99: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf90e5e319d446d8a4a62c941cad401e",
      "max": 45,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_87efc1da8c3b4879a1148935a73fb1b7",
      "value": 45
     }
    },
    "72767f3c61574b7dbe080a01bcdace49": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85c3b1a8cd174ec9ba6f0c7f9b6bb98c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "87efc1da8c3b4879a1148935a73fb1b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "881713e41393412d95ee3207e34e40d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "Epoch 9:  11%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42da303a422d4800926b2e80d7b5a26b",
      "max": 359,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9436c1b792ce4d7bb44367a85c0cbe1a",
      "value": 40
     }
    },
    "8bea138c3ab442bb86b744f810d71767": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c50de042d4ef4ae19f0be884b1acc898",
       "IPY_MODEL_bfa3287788244810a88ae475155cde83"
      ],
      "layout": "IPY_MODEL_ee1598bfc7db4ba4bea30daaf4bf0611"
     }
    },
    "9436c1b792ce4d7bb44367a85c0cbe1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ace5dc252dec4d1ab33a3e9a47b20d59": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72767f3c61574b7dbe080a01bcdace49",
      "placeholder": "​",
      "style": "IPY_MODEL_b8c94a1ad9e34924984c7c1b44becccc",
      "value": " 40/359 [00:33&lt;04:26,  1.20it/s, loss=25.3, v_num=4, g_loss=50, d_loss=0.5]"
     }
    },
    "b8c94a1ad9e34924984c7c1b44becccc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bfa3287788244810a88ae475155cde83": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50c7495608d44921a0c729609106459c",
      "placeholder": "​",
      "style": "IPY_MODEL_2fc48807178e4b1daa98cae6335ba274",
      "value": " 45/45 [10:14&lt;00:00, 13.67s/it, loss=0.767, v_num=0, g_loss=0.795, d_loss=0.606]"
     }
    },
    "c50de042d4ef4ae19f0be884b1acc898": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Epoch 99: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eebaddb891ac4356af67f46d931acff0",
      "max": 45,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_219c85a79f7a4c27865536ef90360b84",
      "value": 45
     }
    },
    "cf90e5e319d446d8a4a62c941cad401e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb09c5f038cc4168ad38f0311efa1b37": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "ee1598bfc7db4ba4bea30daaf4bf0611": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "eebaddb891ac4356af67f46d931acff0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
